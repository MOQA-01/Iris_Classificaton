{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris Identification Project\n",
    "Table of Contents:\n",
    "\n",
    "Introduction\n",
    "Setup and Prerequisites\n",
    "Dataset Description\n",
    "Data Preprocessing\n",
    "Model Building\n",
    "Support Vector Machine (SVM)\n",
    "Hyperparameter Tuning with GridSearchCV\n",
    "Random Forest for Comparison\n",
    "Model Evaluation\n",
    "Conclusion\n",
    "Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "The Iris dataset is a well-known dataset in machine learning for classification tasks. The goal of this project is to build a model that classifies iris flowers into three species (Setosa, Versicolor, Virginica) based on their features.\n",
    "\n",
    "In this notebook, we'll use Support Vector Machine (SVM) and optimize it using GridSearchCV. For comparison, we’ll also apply Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setup and Prerequisites\n",
    "We'll need to install and import a few libraries before we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Dataset Description\n",
    "We will use the Iris dataset from sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Features (sepal length, sepal width, petal length, petal width)\n",
    "X = iris.data\n",
    "\n",
    "# Target (Setosa, Versicolour, Virginica)\n",
    "y = iris.target\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, iris.target_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Preprocessing\n",
    "Before training the model, we need to split the dataset and standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Support Vector Machine (SVM)\n",
    "\n",
    "We will start by training an SVM model with a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an SVM classifier with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the SVM model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"SVM Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Now, we will optimize the SVM model using GridSearchCV to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the SVM model\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=5)\n",
    "\n",
    "# Fit the model using grid search\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and evaluate\n",
    "print(f\"Best Parameters: {grid.best_params_}\")\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_grid = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the optimized SVM\n",
    "optimized_accuracy = accuracy_score(y_test, y_pred_grid)\n",
    "print(f\"Optimized SVM Accuracy: {optimized_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Random Forest for Comparison\n",
    "\n",
    "For comparison, let's build a Random Forest model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Model Evaluation\n",
    "Let's evaluate the performance of both models using confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for Optimized SVM\n",
    "cm_svm = confusion_matrix(y_test, y_pred_grid)\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Optimized SVM')\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. k-Nearest Neighbors (k-NN)\n",
    "k-Nearest Neighbors is a simple, instance-based learning algorithm that classifies data points based on the classes of their nearest neighbors in the feature space. Let's train a k-NN model and compare it with our existing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 Training k-NN Model\n",
    "\n",
    "We will use sklearn.neighbors.KNeighborsClassifier to train the k-NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the k-NN classifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # Default value of k=5\n",
    "\n",
    "# Train the model\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of k-NN model\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"k-NN Accuracy: {knn_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.2 Confusion Matrix for k-NN\n",
    "\n",
    "We will also plot the confusion matrix for the k-NN model to visualize its classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for k-NN\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Purples', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - k-NN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Comparison of Models\n",
    "\n",
    "Now that we have trained three models—SVM, Random Forest, and k-NN—let’s compare their accuracies to understand which one performs best on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy of all models\n",
    "print(f\"Optimized SVM Accuracy: {optimized_accuracy*100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy*100:.2f}%\")\n",
    "print(f\"k-NN Accuracy: {knn_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1 Code for k-NN Hyperparameter Tuning (Optional)\n",
    "\n",
    "If you'd like to fine-tune the value of k for the k-NN model using GridSearchCV,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for k-NN\n",
    "param_grid_knn = {'n_neighbors': np.arange(1, 21)}  # Search for optimal k between 1 and 20\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, verbose=2)\n",
    "grid_knn.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters for k-NN\n",
    "print(f\"Best k for k-NN: {grid_knn.best_params_}\")\n",
    "\n",
    "# Predict using the optimized k-NN model\n",
    "y_pred_knn_optimized = grid_knn.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy of the optimized k-NN model\n",
    "knn_optimized_accuracy = accuracy_score(y_test, y_pred_knn_optimized)\n",
    "print(f\"Optimized k-NN Accuracy: {knn_optimized_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Dimensionality Reduction Using PCA\n",
    "Principal Component Analysis (PCA) is a technique that can help reduce the number of features in the dataset by transforming it into a smaller set of uncorrelated components. This is especially useful for high-dimensional datasets, and it can improve model interpretability, reduce overfitting, and sometimes enhance performance.\n",
    "\n",
    "Let's apply PCA to the Iris dataset and see if reducing the number of features helps maintain or improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.1 Applying PCA to Reduce Dimensionality\n",
    "\n",
    "We'll reduce the feature space from 4 dimensions to 2 dimensions (since the Iris dataset has only 4 features, this will allow us to visualize the data more easily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce the dataset to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Print the explained variance ratio to see how much variance is retained\n",
    "print(f\"Explained Variance Ratio: {pca.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.2 Training SVM, Random Forest, and k-NN with PCA-transformed Data\n",
    "\n",
    "Now, let's retrain our models on the PCA-transformed data and compare their performance with the original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM on PCA-transformed data\n",
    "svm_pca_model = SVC(kernel='linear')\n",
    "svm_pca_model.fit(X_train_pca, y_train)\n",
    "y_pred_svm_pca = svm_pca_model.predict(X_test_pca)\n",
    "svm_pca_accuracy = accuracy_score(y_test, y_pred_svm_pca)\n",
    "print(f\"SVM Accuracy after PCA: {svm_pca_accuracy*100:.2f}%\")\n",
    "\n",
    "# Train Random Forest on PCA-transformed data\n",
    "rf_pca_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_pca_model.fit(X_train_pca, y_train)\n",
    "y_pred_rf_pca = rf_pca_model.predict(X_test_pca)\n",
    "rf_pca_accuracy = accuracy_score(y_test, y_pred_rf_pca)\n",
    "print(f\"Random Forest Accuracy after PCA: {rf_pca_accuracy*100:.2f}%\")\n",
    "\n",
    "# Train k-NN on PCA-transformed data\n",
    "knn_pca_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca_model.fit(X_train_pca, y_train)\n",
    "y_pred_knn_pca = knn_pca_model.predict(X_test_pca)\n",
    "knn_pca_accuracy = accuracy_score(y_test, y_pred_knn_pca)\n",
    "print(f\"k-NN Accuracy after PCA: {knn_pca_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Comparison and Conclusion after PCA\n",
    "\n",
    "By applying PCA, we reduced the dimensionality of the dataset while retaining most of the variance (information) in the data. Here’s how the models performed on the PCA-transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"SVM Accuracy after PCA: {svm_pca_accuracy*100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy after PCA: {rf_pca_accuracy*100:.2f}%\")\n",
    "print(f\"k-NN Accuracy after PCA: {knn_pca_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results:\n",
    "\n",
    "SVM performed similarly after PCA, maintaining high accuracy (~97-98%).\n",
    "Random Forest and k-NN also performed well, with slight variation depending on the number of components selected during PCA.\n",
    "Reducing the features from 4 to 2 didn't drastically reduce the accuracy, indicating that the dataset can be simplified while still maintaining strong classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Conclusion\n",
    "After performing dimensionality reduction using PCA, we conclude:\n",
    "\n",
    "PCA can be beneficial when dealing with high-dimensional datasets, improving model interpretability and reducing computational complexity.\n",
    "For the Iris dataset, we found that reducing the number of features to 2 still retained a significant portion of the variance, with the models performing almost as well as with the full feature set.\n",
    "SVM, Random Forest, and k-NN models performed comparably after applying PCA, making them suitable for classification tasks even with reduced feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Future Improvements\n",
    "\n",
    "Further PCA Tuning: You can experiment with keeping more components (e.g., 3) to retain even more variance.\n",
    "Other Dimensionality Reduction Methods: Techniques such as t-SNE or LDA (Linear Discriminant Analysis) might further improve classification by finding the best projections of the data for specific classes.\n",
    "Explore Other Classifiers: Continuing to experiment with different algorithms (e.g., XGBoost or Neural Networks) might yield even better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
